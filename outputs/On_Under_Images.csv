,Preposition,Accuracy,Count,Dataset,Model,Seed
0,on,0.8571428571428571,21,On_Under_Images,openai-clip:ViT-B/32,1
1,under,0.1428571428571428,21,On_Under_Images,openai-clip:ViT-B/32,1
0,on,0.6666666666666666,21,On_Under_Images,openai-clip:ViT-L/14,1
1,under,0.4285714285714285,21,On_Under_Images,openai-clip:ViT-L/14,1
0,on,0.5238095238095238,21,On_Under_Images,NegCLIP,1
1,under,0.7142857142857143,21,On_Under_Images,NegCLIP,1
0,on,0.0476190476190476,21,On_Under_Images,laion-clip:roberta-ViT-B/32,1
1,under,0.9523809523809524,21,On_Under_Images,laion-clip:roberta-ViT-B/32,1
0,on,0.238095238095238,21,On_Under_Images,blip-coco-base,1
1,under,1.0,21,On_Under_Images,blip-coco-base,1
0,on,0.8571428571428571,21,On_Under_Images,xvlm-pretrained-16m,1
1,under,0.8571428571428571,21,On_Under_Images,xvlm-pretrained-16m,1
0,on,0.9047619047619048,21,On_Under_Images,xvlm-pretrained-16m,1
1,under,1.0,21,On_Under_Images,xvlm-pretrained-16m,1
0,on,0.9047619047619048,21,On_Under_Images,xvlm-pretrained-16m,1
1,under,1.0,21,On_Under_Images,xvlm-pretrained-16m,1
